
<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0" />
    <meta name="google-site-verification" content="VXixwtxrf--qUV1swfStEg9jOGPKgUm6C3Ub_vouqmc" />
    <title>MecBar | The Mec Evolution </title>
    <link rel="icon" href="foto/favicon.ico"/>
    <!-- CSS  -->
    <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection" />
    
    <script src="js/jquery-3.js"></script>
    
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
   
    <link href="https://fonts.googleapis.com/css?family=Raleway|Satisfy" rel="stylesheet">
    <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection" />

    <link href="css/style2.css" type="text/css" rel="stylesheet" media="screen,projection" />
    <link href="css/navbar.css" type="text/css" rel="stylesheet" media="screen,projection" />
 
</head>
<header>
    <nav>
        <li class="nav-wrapper back-home" id="head">
            <a href="http://www.mecbar.com/" class="brand-logo mecbar">
                <?xml version="1.0" encoding="UTF-8" standalone="no"?>
                <svg
                   xmlns:osb="http://www.openswatchbook.org/uri/2009/osb"
                   xmlns:dc="http://purl.org/dc/elements/1.1/"
                   xmlns:cc="http://creativecommons.org/ns#"
                   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
                   xmlns:svg="http://www.w3.org/2000/svg"
                   xmlns="http://www.w3.org/2000/svg"
                   xmlns:xlink="http://www.w3.org/1999/xlink"
                   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
                   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
                   width="156"
                   height="64"
                   viewBox="0 0 12.3825 5.08"
                   version="1.1"
                   id="svg8"
                   inkscape:version="1.0 (6e3e5246a0, 2020-05-07)"
                   sodipodi:docname="mecbar.svg">
                  <defs
                     id="defs2">
                    <linearGradient
                       osb:paint="solid"
                       id="linearGradient2948">
                      <stop
                         id="stop2946"
                         offset="0"
                         style="stop-color:#c6126a;stop-opacity:1;" />
                    </linearGradient>
                    <linearGradient
                       osb:paint="solid"
                       id="linearGradient875">
                      <stop
                         id="stop873"
                         offset="0"
                         style="stop-color:#babd06;stop-opacity:1;" />
                    </linearGradient>
                    <linearGradient
                       id="linearGradient3253"
                       osb:paint="solid">
                      <stop
                         style="stop-color:#878734;stop-opacity:1;"
                         offset="0"
                         id="stop3251" />
                    </linearGradient>
                    <linearGradient
                       inkscape:collect="always"
                       id="linearGradient3093">
                      <stop
                         style="stop-color:#c6126a;stop-opacity:1;"
                         offset="0"
                         id="stop3089" />
                      <stop
                         style="stop-color:#c6126a;stop-opacity:0;"
                         offset="1"
                         id="stop3091" />
                    </linearGradient>
                    <linearGradient
                       id="linearGradient3071"
                       osb:paint="solid">
                      <stop
                         style="stop-color:#c6126a;stop-opacity:1;"
                         offset="0"
                         id="stop3069" />
                    </linearGradient>
                    <rect
                       x="38.182308"
                       y="80.104469"
                       width="51.135052"
                       height="30.274338"
                       id="rect18630" />
                    <rect
                       x="160.31746"
                       y="77.487633"
                       width="134.35785"
                       height="60.829021"
                       id="rect18624" />
                    <rect
                       x="70.354279"
                       y="28.223705"
                       width="103.27115"
                       height="64.638908"
                       id="rect18618" />
                    <rect
                       x="32.55666"
                       y="25.199898"
                       width="5.2916665"
                       height="43.089287"
                       id="rect18612" />
                    <rect
                       x="34.029621"
                       y="65.792862"
                       width="93.441322"
                       height="26.673161"
                       id="rect18606" />
                    <rect
                       x="10.284417"
                       y="19.486744"
                       width="193.27823"
                       height="129.88597"
                       id="rect18596" />
                    <linearGradient
                       inkscape:collect="always"
                       xlink:href="#linearGradient3093"
                       id="linearGradient3101"
                       gradientUnits="userSpaceOnUse"
                       x1="3.1346149"
                       y1="23.792595"
                       x2="78.606865"
                       y2="23.792595" />
                    <filter
                       height="1"
                       y="-2.5183475e-16"
                       width="1"
                       x="-7.1910066e-17"
                       style="color-interpolation-filters:sRGB"
                       inkscape:label="Blend"
                       id="filter3198">
                      <feBlend
                         in2="SourceGraphic"
                         mode="multiply"
                         result="fbSourceGraphic"
                         id="feBlend3196" />
                      <feColorMatrix
                         result="fbSourceGraphicAlpha"
                         in="fbSourceGraphic"
                         values="0 0 0 -1 0 0 0 0 -1 0 0 0 0 -1 0 0 0 0 1 0"
                         id="feColorMatrix3200" />
                      <feBlend
                         in2="fbSourceGraphic"
                         id="feBlend3202"
                         mode="multiply"
                         result="blend"
                         in="fbSourceGraphic" />
                      <feGaussianBlur
                         id="feGaussianBlur869"
                         stdDeviation="6.7542215e-15" />
                    </filter>
                    <linearGradient
                       gradientUnits="userSpaceOnUse"
                       y2="9.1505461"
                       x2="228.5569"
                       y1="9.1505461"
                       x1="3.1346149"
                       id="linearGradient2950"
                       xlink:href="#linearGradient2948"
                       inkscape:collect="always" />
                    <linearGradient
                       y2="9.1505461"
                       x2="228.5569"
                       y1="9.1505461"
                       x1="3.1346149"
                       gradientTransform="translate(2.3455617,-9.683066)"
                       gradientUnits="userSpaceOnUse"
                       id="linearGradient3064"
                       xlink:href="#linearGradient2948"
                       inkscape:collect="always" />
                  </defs>
                  <sodipodi:namedview
                     id="base"
                     pagecolor="#ffffff"
                     bordercolor="#666666"
                     borderopacity="1.0"
                     inkscape:pageopacity="0.0"
                     inkscape:pageshadow="2"
                     inkscape:zoom="0.7"
                     inkscape:cx="355.19772"
                     inkscape:cy="45.714286"
                     inkscape:document-units="mm"
                     inkscape:current-layer="layer1-6"
                     inkscape:document-rotation="0"
                     showgrid="false"
                     inkscape:window-width="1324"
                     inkscape:window-height="747"
                     inkscape:window-x="36"
                     inkscape:window-y="21"
                     inkscape:window-maximized="1"
                     units="px"
                     inkscape:snap-nodes="true"
                     inkscape:object-paths="true"
                     scale-x="0.3" />
                  <metadata
                     id="metadata5">
                    <rdf:RDF>
                      <cc:Work
                         rdf:about="">
                        <dc:format>image/svg+xml</dc:format>
                        <dc:type
                           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
                        <dc:title></dc:title>
                      </cc:Work>
                    </rdf:RDF>
                  </metadata>
                  <g
                     inkscape:label="Layer 1"
                     inkscape:groupmode="layer"
                     class="io"
                     id="layer1">
                    <rect
                       ry="7.1295023"
                       rx="10.726023"
                       y="-2.8428149"
                       x="-4.4855061"
                       height="21.245117"
                       width="20.103241"
                       id="rect2824"
                       style="opacity:0;fill:#c6126a;stroke:#babd06;stroke-width:0.0794996;stroke-opacity:0.00424254" />
                    <g
                       inkscape:label="Layer 1"
                       id="layer1-6"
                       transform="matrix(0.16152221,0.0020121,0,0.13626925,0.08059628,0.86541284)"
                       style="mix-blend-mode:normal;fill:url(#linearGradient2950);fill-opacity:1;stroke-width:6.74038;filter:url(#filter3198);image-rendering:optimizeQuality">
                      <text
                         xml:space="preserve"
                         id="text18594"
                         style="font-style:normal;font-weight:normal;font-size:10.5833px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;white-space:pre;shape-inside:url(#rect18596);fill:url(#linearGradient2950);fill-opacity:1;stroke:none;stroke-width:1.78339px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;" />
                      <text
                         xml:space="preserve"
                         style="font-style:oblique;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:25.0978px;line-height:125%;font-family:Purisa;-inkscape-font-specification:'Purisa, Oblique';font-variant-ligatures:normal;font-variant-caps:normal;font-variant-numeric:normal;font-variant-east-asian:normal;letter-spacing:0px;word-spacing:0px;fill:url(#linearGradient3064);fill-opacity:1;stroke:none;stroke-width:1.78339px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
                         x="-4.0898471"
                         y="24.876211"
                         id="text18602"
                         transform="matrix(0.75566848,-0.20637119,0.34609071,1.2288151,0,0)"><tspan
                   sodipodi:role="line"
                   id="tspan18600"
                   x="-4.0898471"
                   y="24.876211"
                   style="font-style:oblique;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:25.0978px;font-family:Purisa;-inkscape-font-specification:'Purisa, Oblique';font-variant-ligatures:normal;font-variant-caps:normal;font-variant-numeric:normal;font-variant-east-asian:normal;fill:url(#linearGradient3064);fill-opacity:1;stroke-width:1.78339px">MecBar</tspan>        <animate
                   style="fill-opacity:1;fill:url(#linearGradient3064)"
                   begin="0s;light_2.end"
                   dur="1s"
                   values="1;0"
                   attributeName="fill-opacity"
                   id="light_1" />       <animate
                   style="fill-opacity:1;fill:url(#linearGradient3064)"
                   begin="light_1.end"
                   dur="1s"
                   values="0;1"
                   attributeName="fill-opacity"
                   id="light_2" />           </text>
                      <text
                         xml:space="preserve"
                         id="text18604"
                         style="font-style:normal;font-weight:normal;font-size:10.5833px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;white-space:pre;shape-inside:url(#rect18606);fill:url(#linearGradient2950);fill-opacity:1;stroke:none;stroke-width:1.78339px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;" />
                      <text
                         xml:space="preserve"
                         id="text18610"
                         style="font-style:normal;font-weight:normal;font-size:10.5833px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;white-space:pre;shape-inside:url(#rect18612);fill:url(#linearGradient2950);fill-opacity:1;stroke:none;stroke-width:1.78339px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;" />
                      <text
                         xml:space="preserve"
                         id="text18616"
                         style="font-style:normal;font-weight:normal;font-size:10.5833px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;white-space:pre;shape-inside:url(#rect18618);fill:url(#linearGradient2950);fill-opacity:1;stroke:none;stroke-width:1.78339px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;" />
                      <text
                         xml:space="preserve"
                         id="text18622"
                         style="font-style:normal;font-weight:normal;font-size:10.5833px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;white-space:pre;shape-inside:url(#rect18624);fill:url(#linearGradient2950);fill-opacity:1;stroke:none;stroke-width:1.78339px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;" />
                      <text
                         xml:space="preserve"
                         id="text18628"
                         style="font-style:normal;font-weight:normal;font-size:12.7px;line-height:125%;font-family:sans-serif;text-align:justify;letter-spacing:0px;word-spacing:0px;white-space:pre;shape-inside:url(#rect18630);fill:url(#linearGradient2950);fill-opacity:1;stroke:none;stroke-width:1.78339px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;" />
                      <animate
                         style="fill-opacity:1;fill:url(#linearGradient2950)"
                         begin="0s;light_2.end"
                         dur="1s"
                         values="1;0"
                         attributeName="fill-opacity"
                         id="light_1" />
                      <animate
                         style="fill-opacity:1;fill:url(#linearGradient2950)"
                         begin="light_1.end"
                         dur="1s"
                         values="0;1"
                         attributeName="fill-opacity"
                         id="light_2" />
                    </g>
                    <rect
                       ry="7.1295042"
                       rx="10.726021"
                       y="-2.8428149"
                       x="-4.4855061"
                       height="16.413473"
                       width="25.987757"
                       id="rect871"
                       style="opacity:0;fill:#c6126a;stroke:#babd06;stroke-width:0.0794996;stroke-opacity:0.00424254" />
                  </g>
                  <style
                     id="style43">
                           #tspan18600 {
                               
                              animation-name: mecOpacity;
                              animation-duration: 2s;
                              animation-iteration-count: infinite;
                              }
                              @keyframes mecOpacity {
                              0%   { fill-opacity:1 }
                                50%  {fill-opacity :0.1; }
                            100% { fill-opacity: 1; }
                              }
                    </style>
                </svg>
                </a>
            <a href="#" data-activates="mobile-demo" class="button-collapse"><i class="material-icons">menu</i></a>

            <ul class="right hide-on-med-and-down">
                <!--  <li> <a class="btn" onclick="Materialize.toast('Hello', 4000, 'Ciao' ,4000 )">Touch Me</a> </li>
              -->

              <li><a href="http://www.mecbar.com/#Ablog">Blog</a></li>
              <li><a href="http://www.mecbar.com/#Ablock">Blockchain</a></li>
              <li><a href="http://www.mecbar.com/#quantum">Quantum</a></li>
              <li><a href="http://www.mecbar.com/#Amachine">Machine Learning</a></li>
              <li><a href="http://www.mecbar.com/#Alinux">Linux</a></li>
              <li><a href="http://www.mecbar.com/#Aus">Contatti</a></li>
                <li>
                    <a href=""> </a>
                </li>
                <li>
                    <a href=""> </a>
                </li>
            </ul>
          
              <ul class="side-nav" id="mobile-demo">
               <li><a href="http://www.mecbar.com/#Ablog">Blog</a></li>
              <li><a href="http://www.mecbar.com/#Ablock">Blockchain</a></li>
                 <li><a href="http://www.mecbar.com/#quantum">Quantum</a></li>
              <li><a href="http://www.mecbar.com/#Amachine">Machine Learning</a></li>
              <li><a href="http://www.mecbar.com/#Alinux">Linux</a></li>
              <li><a href="http://www.mecbar.com/#Aus">Contatti</a></li>
            </ul>
        
            </li>
    </nav>
</header>
<script>
    MathJax = {
      loader: {load: ['input/asciimath', 'output/chtml']}
    }
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<body>
<div class="center titolo">
    <i>Text Generation con Recurrent Neural Network - LSTM  PyTorch </i>
 </div>

<div class="testo">
In questo post vediamo come è possibile generare automaticamento un testo tramite un algoritmo di 
Deep Learning tramite il metodo RNN e più specificatamente il Long Short Time Memory. <br>
Partiamo dalla rappresentazione grafica di un Recurrent Neural Network.<br><br>

<img src="foto/RNN.png" width="80%" height="200px"><br><br>

Nella figura x è l'input, y è l'output e h sono gli hidden layers e v indica la ricorsione dove l'output 
diviene poi l'input del ciclo sucessivo.  <br><br>
Nella parte dx della figura lo stesso procedimento viene rappresentato in successione con indicati 
i diversi tempi del processo in cui x,y e h hanno t-1 poi t e poi t+1. <br><br>
u identifica i weights del cell state precedente che applicati ad x vengono elaborati in h.  <br>
w identifica i weights che applicati al risultato di h determina y.  <br><br>
Le parole The Computational Graph che nell'esempio che verrà proposto in seguito saranno input qui vengono inserite 
nella figura per vedere in modo chiaro cosa avviene, infatti "The" è il primo output che nel passaggio 
successivo diviene l'input che poi produce in output "Computational" e così via...   <br>
<br>
Il modello LSTM è un particolare modello RNN che risolve il problema del vanishing gradient. Qui proponiamo una rappresentazione 
grafica. <br>   <br>  

<img src="foto/lstm.png" width="50%" height="350px"><br>
<br> 
Come possiamo osservare sono presenti come input C(Cell state) e h state cioè l'output precedente 
e poi l'input specifico x.  <br> <br>
All'interno del blocco si osservano i diversi full connection layers Softmax activation e Tahn activation 
con i dati che seguono le indicazioni delle frecce e con gli elementi di moltiplicazione o somma. <br> 
In output otteniamo C state e h che poi diventano input per il blocco successivo come possiamo intuire 
vedendo le immagini sottostanti dove vengono indicate le fasi temporali con t-1, t e t+1.<br> <br>   


<img src="foto/lstm0.png" width="33%" height="350px">
<img src="foto/lstm.png" width="33%" height="350px">
<img src="foto/lstm2.png" width="33%" height="350px">

<br> <br> <br>

Dopo aver visto una sommaria descrione del LSTM ora viene proposto un modello in Python 
di un algoritmo creato con il framework PyTorch che inserendo in input le parole iniziali di una frase 
creerà la parte restante della frase. <br> <br> 
L'essenza del modello è quella di apprendere dalle parole che vengono inserite in input per cui ad ogni parola inserita in input il modello ci ritorna una parola 
in output che poi viene riutilizzata in input nel passaggio successivo e così via fino al raggiungimento 
della lunghezza prefissata.<br> <br> 

<div class="model2">


import torch<br>
import torch.nn as nn<br>
import torch.nn.functional as F<br>

import numpy as np<br>
from collections import Counter<br>
import os<br>

<br><br>
   
sequenceDim=64<br>
batch_size=16<br>
embeddingDim=64<br>
lstmDim=64<br>
numLayers = 3<br>
<br><br><br>
<span class="rosso">def </span> getData(input, out, batch_size, sequenceDim,nb):<br>
<span style="margin-left:30px"> for i in range(0, nb * sequenceDim, sequenceDim):</span><br>
    <span style="margin-left:60px"> yield input[:, i:i+sequenceDim], out[:, i:i+sequenceDim]</span><br>

    <br><br><br>
    <span class="rosso">class</span> generaText(nn.Module):<br><br>
<span style="margin-left:30px"> <span class="verde">def </span>  __init__(self, nParole, sequence, embeddingDim, lstmDim,numLayers=1 ):</span><br>
  <span style="margin-left:60px"> super(generaText, self).__init__()</span><br>
 <span style="margin-left:60px">self.sequence = sequence</span><br>
  <span style="margin-left:60px">self.lstmDim = lstmDim</span><br>
  <span style="margin-left:60px">self.numLayers = numLayers</span><br>
  <span style="margin-left:60px">self.embedding = nn.Embedding(nParole, embeddingDim)</span><br>
  <span style="margin-left:60px"> self.lstm = nn.LSTM(input_size=embeddingDim,</span><br>
<span style="margin-left:90px"> hidden_size=self.lstmDim,</span><br>
  <span style="margin-left:90px"> num_layers=self.numLayers,</span><br>
 <span style="margin-left:90px"> batch_first=True,</span><br>
 <span style="margin-left:90px"> dropout=0.1)</span><br>

           
 <span style="margin-left:60px">self.out = nn.Linear(lstmDim, nParole)</span><br>
 <br>
<span style="margin-left:30px"><span class="verde"> def </span> forward(self, x, in_state):</span><br>
   <span style="margin-left:60px"> embedding = self.embedding(x)</span><br>
  <span style="margin-left:60px">output, out_state = self.lstm(embedding, in_state)</span><br>
 <span style="margin-left:60px">logits = self.out(output)</span><br>
    
  <span style="margin-left:60px">return logits, out_state   </span><br>
  <br>
    
 <span style="margin-left:30px"> <span class="verde">  def </span> init_state(self, sequence=1):</span><br>
 <span style="margin-left:60px"> return (torch.zeros(self.numLayers, sequence, self.lstmDim),</span><br>
    <span style="margin-left:60px"> torch.zeros(self.numLayers, sequence, self.lstmDim)) </span><br>

    <br><br>
#IMPORT TESTO <br><br><br><br>

text    # vediamo il testo da cui estrarre le parole da inserire in input nel modello <br>
<br>
backpropagation is a backward propagation of errors and is a powerful tool of the deep learning. with the gradient descent the backpropagation reduce the cost function and the time of execution. we now talk about of calculate the gradient descent. 
with the gradient descent we want find the weights that minimize the errors  the cost function  through some iterations for search the minimum. there are some method  we see the principal  the batch gradient descent  the stochastic gradient descent and mini batch gradient descent. the first  the batch gradient descent  is a deterministic method that start always with the same data and produce the same outcome. it calculate the cost function of all the input data and then update the weights through the backpropagation. this process is very expensive in time and resources for load all data in memory  especially as data gets big  for found the best cost function. the stochastic gradient descent  shortened sgd  is a stochastic method because the outcome is not always the same. with sgd calculate the cost function of 1 input data and then upgrade the weights. this for every input data. this method is faster because not need of very resources and its most used when we have very input data.
the mini batch gradient descent is recent and its balance between the first 2 method. we get derivative for a ”small” set of points  tipical mini batch size is 16 or 32  then update the weights and backpropagate it.
an epoch refers to a single forward pass through all of the training data:
  in batch gradient descent there is 1 step for epoch
  in stochastic gradient descent there are n steps for epoch were n is the training set size
  in mini batch gradient descent there are n steps for epoch were n is equal to training set size/batch size 16 or 32 
naturally epoch is another important parameter for artificial neural network.
the epoch
for prevent overfitting there are some tecnics call optimizer. one of this is dropout and most popular are adam and rmsprop. the optimizer are variants to update the weights to give a better performance of neural network. rmsprop is a adaptive learning rate method is a variant of adagrad method for upgrade the weights its modulates the learning rate of each weight in base of their gradient value equalizing the effect.
adam is a new method and most utilised. its a variant of rmsprop. with dropout we avoid overfitting reducing the number of nodes setting to 0 the value of nodes selected randomly. so the hidden layers are in lower number respect the input layers based to dropout parameter inserted into the neural network. 
in the above image the graph of search of minimum cost function from 2 to 3 dimensional space.
the formula for calculate the cost function for every input data and for all the input data
in this section we calculate the cost function of a simple linear regression function y = ax + b with the adjusted r squared method.
the adjusted r squared is a method for calculate the error of the predictions value of a regression compared to the actuel value. its adjusted because the squared value is always positive so its divided for 2.
in this example the value of coefficients with a minimum cost function is a = 1 and b = 1 and the model for the regression is y = x + 1 .
in this example we simulated three cases with the coefficients a and b  1 1    2 1  and  2 2 . the results is in the column p1  p2 and p3. then we calculate their cost function in column c1  c2 and c3. doing the sum we have the total cost function and we take the coefficients with the minimum cost function. if we use the backpropagation we take a part of the errors to select the learning rate that is a parameter very important of the machine learning. we select a very small learning rate  max 0.05  and update the weights.
here in the schema we show the difference between the batch gradient descent and the stochastic gradient descent. in the sgd after calculate even cost function we update the weights while in the batch we update the weights only after we calculate the total cost function. in mini batch gradient descent we divide the training data into block with the batch size dimension. after we calcolate the cost function of one block and upgrade the weights.
for calculate the gradient we introduce the computational graph. the computational graph is a method for represent a process in some steps  a data flow graph  were represent the operation in the chart. each step corresponds to a simple operation. there are some inputs and produces some output as a function. in the image the computational graph. the calculation of the local gradient is influenced by the upstream gradient in every step into the backward pass process until the final cost function.
the gradient calculated come back to the hidden layers and then upgrade the weights and the process restart with the forward pass.
the computational graph is used for feedforward and for backforward. now a simple example of a computational graph that represent a function. we calculate the value of the function in black and the gradient in red with the rules show in the image.
the function f x y z  = f x+y z with x = 3  y =  4  z = 7 and we insert w =  x + y  
in the graph we can see how calculate the gradients with the local gradient and the chain rule. the chain rule tells us how find the derivative of a composite function.
the multiply gate return us the gradient with the following rule:
  the gradient of x is equal to upstream gradient multiply by y value in the forward pass
  the gradient of y is equal to upstream gradient multiply by x value in the forward pass.
the add gate return us the same gradient value of upstream gradient to x and y gradient.
the max gate return us the gradient with the following rule:
  who has the greater value in forward pass between x and y take the gradient equal to the upstream and the other take the value equal to zero
in the graph we can see how calculate the gradients with the local gradient and the chain rule. the chain rule tells us how find the derivative of a composite function.
the multiply gate return us the gradient with the following rule:
  the gradient of x is equal to upstream gradient multiply by y value in the forward pass
  the gradient of y is equal to upstream gradient multiply by x value in the forward pass.
the add gate return us the same gradient value of upstream gradient to x and y gradient.
the max gate return us the gradient with the following rule:
  who has the greater value in forward pass between x and y take the gradient equal to the upstream and the other take the value equal to zero
in practice we represent a logistic regression how you can see in the project of the bank marketing. here we have only 2 input layer x1 and x2 but the process for calculate the backpropagation is the same. in the graph in red the value of gradient from right to left that with the backpropagation we update the weights of neural network. 

<br><br><br>
text = text.split()<br>
<br>
bow = Counter(text)   # bag of words <br>
vocab = sorted(bow, key=bow.get, reverse=True)<br>
nParole=len(bow)  # Numero parole inserite in text <br>
numero_to_parola = {k: w for k, w in enumerate(vocab)}  # assegna un numero ad ogni parola<br>
parola_to_numero = {w: k for k, w in numero_to_parola.items()} # assegna una parola ad ogni numero <br>
<br>
textInNumeri = [parola_to_numero[w] for w in text]<br>
num = int(len(textInNumeri)/(batch_size * sequenceDim))<br>
textInNumeri = textInNumeri[: num * batch_size * sequenceDim]<br>
outTextInNumeri = np.zeros_like(textInNumeri)<br>
outTextInNumeri[:-1] = textInNumeri[1:]<br>
outTextInNumeri[-1] = textInNumeri[0]<br>
textInNumeri = np.reshape(textInNumeri, (batch_size, -1))<br>
outTextInNumeri = np.reshape(outTextInNumeri, (batch_size, -1))<br>
nb = int((textInNumeri.shape[0] * textInNumeri.shape[1])/(sequenceDim * batch_size))<br>

<br>
# creazione modello LSTM<br><br>
lstm = generaText(nParole, sequenceDim,embeddingDim, lstmDim, numLayers)<br>
<br><br>
# creazione loss model ed optimizer <br><br>
lossModel = nn.CrossEntropyLoss()<br>
optimizer = torch.optim.Adam(lstm.parameters(), lr=0.03)<br>
<br><br>

epochs = 10000<br>
iterator = 0<br>
<br><br>

for epoch in range(1,epochs+1):<br>
<br>   
<span style="margin-left:30px">#  estrazioni dati input ed output    </span><br>   
<span style="margin-left:30px">data = getData(textInNumeri, outTextInNumeri, batch_size, sequenceDim, nb)</span><br>
<span style="margin-left:30px"> h_0, c_0 = lstm.init_state( batch_size)  # azzerare cell state e hidden state </span><br>
       
<span style="margin-left:30px">for x, y in data :</span><br>
          
 <span style="margin-left:60px">iterator += 1</span><br>
   <span style="margin-left:60px"># Set model in training mode</span><br>
   <span style="margin-left:60px"> lstm.train()</span><br>

  <span style="margin-left:60px"># Reset all gradients</span><br>
 <span style="margin-left:60px"> optimizer.zero_grad()</span><br>

   <span style="margin-left:60px"># x e y vengono convertiti in Tensor</span><br>
    <span style="margin-left:60px"> x = torch.tensor(x)</span><br>
   <span style="margin-left:60px"> y = torch.tensor(y)</span><br>
            
   <span style="margin-left:60px">logits, (h_0, c_0) = lstm(x, (h_0, c_0))</span><br>
 <span style="margin-left:60px">loss = lossModel(logits.transpose(1, 2), y)</span><br>
   
  <span style="margin-left:60px"> h_0 = h_0.detach()   # no back-propagation per h_0 </span><br>
   <span style="margin-left:60px"> c_0= c_0.detach()   # no back-propagation per c_0 </span><br>
  <span style="margin-left:60px"> # otteniamo il valore loss</span><br>
 <span style="margin-left:60px">loss_value = loss.item()</span><br>
 
<span style="margin-left:60px"> # back-propagation</span><br>
 <span style="margin-left:60px">loss.backward()</span><br>
          
 <span style="margin-left:60px">torch.nn.utils.clip_grad_value_(lstm.parameters(), clip_value=5) # update gradients </span><br>
  <span style="margin-left:60px"> # Update network parameters</span><br>
 <span style="margin-left:60px"> optimizer.step()</span><br>

   <span style="margin-left:60px">if iterator % 100 == 0:</span><br>
  <span style="margin-left:90px"> print(f'Epoch: {epoch  }/{epochs }, '  f' Loss: {loss_value  :2.5f}')</span><br>

  <br><br>

parole_iniziali = ['The','Computational', 'Graph']<br>

parole_iniziali2 = ['The', 'backward']<br>
<br><br>

numero_parole_da_generare = 10<br>
<br><br>               
lstm.eval() # set model per valutazione<br>
h_0, c_0 = lstm.init_state() # set a zero <br>
<br>      
nuovo_testo = parole_iniziali<br>
<br><br>        
for _ in range(numero_parole_da_generare):<br>
<span style="margin-left:30px">parola_generata = parola_to_numero[nuovo_testo[-1].lower()]</span><br>
 <span style="margin-left:30px">   inp = torch.tensor([[parola_generata]])</span><br>
 <span style="margin-left:30px">   output, (h_0, c_0) = lstm(inp, (h_0, c_0))</span><br>
                
 <span style="margin-left:30px">_, topK = torch.topk(output[0], k=3)  # seleziona i primi 3 risultati</span><br>
  <span style="margin-left:30px"> selezione = topK.tolist()</span><br>
  <span style="margin-left:30px">parola_generata = np.random.choice(selezione[0])</span><br>
 <span style="margin-left:30px"> nuovo_testo.append(numero_to_parola[parola_generata])</span><br>
 <br><br>           
 print(' '.join(nuovo_testo))<br><br>

 <br>
 Risultato con input ['The','Computational', 'Graph'] <br><br>
<span class="blu"> Computational Graph in this example we three cases parameter a select very the
</span><br><br>
   Risultato con input ['The', 'backward'] <br><br>
   <span class="blu"> 
   The backward above cost function method in see cases following data can

</span><br>
 




</div>






</div>






</body>
</html>